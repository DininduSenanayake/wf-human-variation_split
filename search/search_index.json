{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"epi2me-labs/wf-human-variation for NeSI Mahuika cluster <p>modified version of epi2me-labs/wf-human-variation        (https://github.com/epi2me-labs/wf-human-variation) to make the workflow compatible with NeSI Mahuika cluster Slurm scheduler</p> 02-ont-guppy-gpu <ul> <li>Refer to NeSI Support documentation https://support.nesi.org.nz/hc/en-gb/articles/4546820344079-ont-guppy-gpu</li> </ul> <p>Dorado</p> <ul> <li>Dorado is available as a module but treat as a R&amp;D application for the moment.</li> <li>NeSI support documentation for Dorado https://support.nesi.org.nz/hc/en-gb/articles/6623692647951-Dorado </li> </ul> 03-ont-bam-merge.slurm <pre><code>#!/bin/bash -e\n\n#SBATCH --account         nesi12345\n#SBATCH --job-name        ont-bam-merge\n#SBATCH --cpus-per-task   16\n#SBATCH --mem             64G\n#SBATCH --time            24:00:00\n\nmodule purge\nmodule load BamTools/2.5.2-GCC-11.3.0\nmodule load Sambamba/0.8.0\n\n# define variables\nWKDIR='/path/to/wkir'                           #For an example : \"/nesi/nobackup/nesi12345/..\"\nSAMPLE='PBXP289487'\n\n# move to working dir\ncd ${WKDIR}/${SAMPLE}/bam\n\n# sambamba\n# sambamba merge -t 4 -p ${SAMPLE}_sorted_merged.bam pass/*.bam\n# bamtools\nls pass/*.bam &gt; bam_list.txt\nbamtools merge -l bam_list.txt -out ${SAMPLE}_merged.bam\nsambamba sort -m 52GB -t 16 ${SAMPLE}_merged.bam -o ${SAMPLE}_sorted_merged.bam\nsambamba index -t 16 ${SAMPLE}_sorted_merged.bam\n</code></pre> 04-ont--human-variation-calling.slurm <ul> <li> <p>Create a file named nextflow_local_override.config with the following content </p> <pre><code>executor {\n$local {\n           cpus = 48\n           memory = \"128 GB\"\n       }\n         }\n\nsingularity {\n  runOptions = \"-B ${TMPDIR}\"\n            }\n</code></pre> </li> <li> <p>Slurm script </p> </li> </ul> <pre><code>#!/bin/bash -e\n#SBATCH --name          ont-human-variation\n#SBATCH --cpus-per-task 48\n#SBATCH --mem           128G\n#SBATCH --partition     milan\n#SBATCH --time          24:00:00\n\nmodule purge\nmodule load Nextflow/22.10.3\nmodule load Singularity/3.10.3\n\n# define variables\nWKDIR='/path/to/workingdir'                           #For an example : \"/nesi/nobackup/nesi12345/..\"\nSAMPLE='PBXP289487'\nMODEL='/path/to/clair3_models/ont_guppy5'\nREFERENCE='/path/to/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta'\nTDREPETS='/path/to/bed/reference/human_GRCh38_no_alt_analysis_set.trf.bed'\nOUTPUT='results'\n\n#Singularity and Nextflow variables\nexport SINGULARITY_TMPDIR=/some/path/in/nobackup/singularity_cache\nexport SINGULARITY_CACHEDIR=$SINGULARITY_TMPDIR\nsetfacl -b \"$SINGULARITY_TMPDIR\"\n\nexport NXF_EXECUTOR=slurm\nexport NXF_SINGULARITY_CACHEDIR=$SINGULARITY_CACHEDIR\n\nNFCONFIG='/path/to/nextflow_local_overide.config'\n# note: created an overide config to provide modified CPU and Memory values\n# change these values if you want to tweak performance based on resources\n\n# move to working dir\ncd ${WKDIR}/${SAMPLE}\n\n# pull nextflow pipeline (if haven't already)\nnextflow run epi2me-labs/wf-human-variation --help\n\n# run Clair3 variant calling\nnextflow -c ${NFCONFIG} run epi2me-labs/wf-human-variation \\\n  -resume \\\n  --threads ${SLURM_CPUS_PER_TASK} \\\n  -profile singularity,local \\\n  --snp --sv \\\n  --phase_vcf \\\n  --use_longphase \\\n  --tr_bed ${TDREPETS} \\\n  --model ${MODEL} \\\n  --bam ./bam/${SAMPLE}_sorted_merged.bam \\\n  --ref ${REFERENCE} \\\n  --out_dir ${OUTPUT}\n</code></pre> 05-ont-whatshap-phase.slurm <pre><code>#!/bin/bash -e\n\n#SBATCH --account         nesi12345\n#SBATCH --job-name        ont-whatshap-phase\n#SBATCH --cpus-per-task   16\n#SBATCH --mem             64G\n#SBATCH --time            24:00:00\n\nmodule purge\nmodule load WhatsHap/1.6-gimkl-2022a-Python-3.10.5\nmodule load SAMtools/1.16.1-GCC-11.3.0\n\n# define variables\nWKDIR='/path/to/workingdir'\nSAMPLE='PBXP289487'\nREFERENCE='/path/to/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta'\nOUTPUT='results'\n\n# move to working dir\ncd ${WKDIR}/${SAMPLE}\n\n# whatshap phase tagging of bam output\nwhatshap haplotag \\\n    --ignore-read-groups \\\n    --output ./bam/${SAMPLE}_sorted_merged.hp.bam  \\\n    --reference ${REFERENCE} \\\n    ${OUTPUT}/all.wf_snp.vcf.gz ./bam/${SAMPLE}_sorted_merged.bam\n# index bam\nsamtools index -@ 16 ./bam/${SAMPLE}_sorted_merged.hp.bam\n\n# Notes:\n# this step phases the data based on the clair3 output and generates a \n# phased bam file. This contains information assigning reads to each\n# haplotype. At this stage the bam is able to loaded into a genome \n# viewer/browser and contains aligned reads, with base modification\n# (methylation) information, as well as the haplotype information.\n</code></pre> 06-ont-methyl-calling.slurm <pre><code>#!/bin/bash -e\n\n#SBATCH --account         nesi12345\n#SBATCH --job-name        ont-bam-merge\n#SBATCH --cpus-per-task   16\n#SBATCH --mem             32G\n#SBATCH --time            08:00:00\n\nmodule purge\nmodule load modbam2bed/0.9.4-GCC-11.3.0\n\n# define variables\nWKDIR='/path/to/workdir'\nSAMPLE='PBXP289487'\nREFERENCE='/path/to/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta'\n\n# move to working dir\ncd ${WKDIR}/${SAMPLE}\n\n# create methylation bed files\nfor HP in 1 2; do\n    modbam2bed \\\n        -e -m 5mC --cpg -t 16 --haplotype ${HP} \\\n        ${REFERENCE} \\\n        ./bam/${SAMPLE}_sorted_merged.hp.bam \\\n        | bgzip -c &gt; ./bed/${SAMPLE}_methylation.hp${HP}.cpg.bed.gz\ndone;\n\n# create an aggregated bed file\nmodbam2bed \\\n  -e -m 5mC --cpg --aggregate -t 16 \\\n  ${REFERENCE} \\\n  ./bam/${SAMPLE}_sorted_merged.hp.bam \\\n  | bgzip -c &gt; ./bed/${SAMPLE}_methylation.aggregated.cpg.bed.gz\n\n# Notes:\n# this step extracts the methylation information from the bam file, generating\n# bed files. There are two processes here, the first creates two bed files, one\n# for each haplotype. These are very useful for exploring allele specific methylation.\n# The second process generates a single aggragated bed file, it merges the\n# haplotype data to give site specific methylation. It should be noted that this is\n# per strand, so some processing will be required to 'collapse' the data to a\n# single CpG site, but this type of work is usually performed in the downstream\\\n# analysis, using packages such as methylkit.\n</code></pre> 07-ont-sv-cutesv.slurm <pre><code>#!/bin/bash -e\n\n#SBATCH --account         nesi12345\n#SBATCH --job-name        ont-csv-cutesv\n#SBATCH --cpus-per-task   16\n#SBATCH --mem             128G\n#SBATCH --time            06:00:00\n\nmodule purge\nmodule load cuteSV/2.0.2-gimkl-2020a-Python-3.8.2\n\n# define variables\nWKDIR='/path/to/workingdir'\nSAMPLE='PBXP289487'\nREFERENCE='/path/to/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta'\nOUTPUT='results'\n\n# move to working dir\ncd ${WKDIR}/${SAMPLE}\nmkdir cutesv_tmp\n\n# cuteSV processing\ncuteSV -t ${SLURM_CPUS_PER_TASK} \\\n  --max_cluster_bias_INS 100 \\\n  --diff_ratio_merging_INS 0.3 \\\n  --max_cluster_bias_DEL 100 \\\n  --diff_ratio_merging_DEL 0.3 \\\n  ./bam/${SAMPLE}_sorted_merged.hp.bam \\\n  ${REFERENCE} \\\n  ${OUTPUT}/${SAMPLE}_sv_cutesv.vcf \\\n  ./cutesv_tmp\n\n# Notes:\n# this step is for evaluation of another structural variant caller, cuteSV. It's \n# often nice to have the ability to compare results between various tools. As \n# SVs are important to this project this step has been included in the process.\n# For other projects it may well be enough to stop after processing step 06.\n# This process outputs a vcf file with the structural variation recorded per\n# line.\n</code></pre>"}]}